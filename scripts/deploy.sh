#!/bin/bash

# AI Travel Planner éƒ¨ç½²è„šæœ¬
# æ”¯æŒå¼€å‘ã€æµ‹è¯•å’Œç”Ÿäº§ç¯å¢ƒçš„ä¸€é”®éƒ¨ç½²

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# ==================== é…ç½®å˜é‡ ====================

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
LOG_FILE="$PROJECT_ROOT/deploy.log"

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# é»˜è®¤é…ç½®
ENVIRONMENT="development"
SKIP_BUILD=false
SKIP_TESTS=false
FORCE_RECREATE=false
CLEANUP_VOLUMES=false
ENABLE_MONITORING=true
ENABLE_LOGGING=true

# ==================== å¸®åŠ©å‡½æ•° ====================

log() {
    echo -e "${GREEN}[$(date +'%Y-%m-%d %H:%M:%S')] $1${NC}" | tee -a "$LOG_FILE"
}

warn() {
    echo -e "${YELLOW}[$(date +'%Y-%m-%d %H:%M:%S')] WARNING: $1${NC}" | tee -a "$LOG_FILE"
}

error() {
    echo -e "${RED}[$(date +'%Y-%m-%d %H:%M:%S')] ERROR: $1${NC}" | tee -a "$LOG_FILE"
}

info() {
    echo -e "${BLUE}[$(date +'%Y-%m-%d %H:%M:%S')] INFO: $1${NC}" | tee -a "$LOG_FILE"
}

# æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
show_help() {
    cat << EOF
AI Travel Planner éƒ¨ç½²è„šæœ¬

ç”¨æ³•: $0 [é€‰é¡¹]

é€‰é¡¹:
    -e, --env ENV              è®¾ç½®ç¯å¢ƒ (development|staging|production) [é»˜è®¤: development]
    -s, --skip-build           è·³è¿‡Dockeré•œåƒæ„å»º
    -t, --skip-tests           è·³è¿‡æµ‹è¯•è¿è¡Œ
    -f, --force-recreate       å¼ºåˆ¶é‡æ–°åˆ›å»ºæ‰€æœ‰å®¹å™¨
    -c, --cleanup-volumes      æ¸…ç†æ‰€æœ‰æ•°æ®å·ï¼ˆå±é™©æ“ä½œï¼ï¼‰
    --no-monitoring            ç¦ç”¨ç›‘æ§æœåŠ¡
    --no-logging               ç¦ç”¨æ—¥å¿—æœåŠ¡
    -h, --help                 æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯

ç¯å¢ƒè¯´æ˜:
    development               å¼€å‘ç¯å¢ƒï¼Œå¯ç”¨çƒ­é‡è½½å’Œè°ƒè¯•åŠŸèƒ½
    staging                   æµ‹è¯•ç¯å¢ƒï¼Œæ¨¡æ‹Ÿç”Ÿäº§é…ç½®
    production                ç”Ÿäº§ç¯å¢ƒï¼Œä¼˜åŒ–æ€§èƒ½å’Œå®‰å…¨æ€§

ç¤ºä¾‹:
    $0                        # ä½¿ç”¨é»˜è®¤å¼€å‘ç¯å¢ƒéƒ¨ç½²
    $0 -e production -f       # ç”Ÿäº§ç¯å¢ƒå¼ºåˆ¶é‡æ–°éƒ¨ç½²
    $0 -s -t                  # è·³è¿‡æ„å»ºå’Œæµ‹è¯•çš„å¿«é€Ÿéƒ¨ç½²

EOF
}

# ==================== ç¯å¢ƒæ£€æŸ¥ ====================

check_prerequisites() {
    log "ğŸ” æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒ..."
    
    # æ£€æŸ¥æ“ä½œç³»ç»Ÿ
    if [[ "$OSTYPE" == "linux-gnu"* ]]; then
        OS="linux"
    elif [[ "$OSTYPE" == "darwin"* ]]; then
        OS="macos"
    elif [[ "$OSTYPE" == "msys" ]] || [[ "$OSTYPE" == "win32" ]]; then
        OS="windows"
    else
        error "ä¸æ”¯æŒçš„æ“ä½œç³»ç»Ÿ: $OSTYPE"
        exit 1
    fi
    
    info "æ£€æµ‹åˆ°æ“ä½œç³»ç»Ÿ: $OS"
    
    # æ£€æŸ¥ Docker
    if ! command -v docker &> /dev/null; then
        error "Docker æœªå®‰è£…ã€‚è¯·è®¿é—® https://docs.docker.com/get-docker/ å®‰è£…"
        exit 1
    fi
    
    # æ£€æŸ¥ Docker ç‰ˆæœ¬
    DOCKER_VERSION=$(docker --version | grep -oE '[0-9]+\.[0-9]+')
    REQUIRED_DOCKER_VERSION="20.10"
    
    if [ "$(printf '%s\n' "$REQUIRED_DOCKER_VERSION" "$DOCKER_VERSION" | sort -V | head -n1)" != "$REQUIRED_DOCKER_VERSION" ]; then
        warn "Docker ç‰ˆæœ¬ ($DOCKER_VERSION) å¯èƒ½è¿‡ä½ï¼Œå»ºè®®ä½¿ç”¨ $REQUIRED_DOCKER_VERSION æˆ–æ›´é«˜ç‰ˆæœ¬"
    fi
    
    # æ£€æŸ¥ Docker Compose
    if ! command -v docker-compose &> /dev/null && ! docker compose version &> /dev/null; then
        error "Docker Compose æœªå®‰è£…ã€‚è¯·å®‰è£… Docker Compose"
        exit 1
    fi
    
    # æ£€æŸ¥ Docker å®ˆæŠ¤è¿›ç¨‹
    if ! docker info &> /dev/null; then
        error "Docker å®ˆæŠ¤è¿›ç¨‹æœªè¿è¡Œã€‚è¯·å¯åŠ¨ Docker"
        exit 1
    fi
    
    # æ£€æŸ¥ç³»ç»Ÿèµ„æº
    check_system_resources
    
    log "âœ… ç¯å¢ƒæ£€æŸ¥å®Œæˆ"
}

check_system_resources() {
    info "æ£€æŸ¥ç³»ç»Ÿèµ„æº..."
    
    # æ£€æŸ¥å†…å­˜
    if [[ "$OS" == "linux" ]]; then
        TOTAL_MEM=$(free -m | awk 'NR==2{printf "%.0f", $2/1024}')
        AVAILABLE_MEM=$(free -m | awk 'NR==2{printf "%.0f", $7/1024}')
    elif [[ "$OS" == "macos" ]]; then
        TOTAL_MEM=$(system_profiler SPHardwareDataType | awk '/Memory/ {print int($2)}')
        AVAILABLE_MEM=$TOTAL_MEM  # ç®€åŒ–å¤„ç†
    fi
    
    info "ç³»ç»Ÿå†…å­˜: ${TOTAL_MEM}GB"
    
    if [ "$TOTAL_MEM" -lt 8 ]; then
        warn "ç³»ç»Ÿå†…å­˜ä¸è¶³ 8GBï¼Œå¯èƒ½å½±å“æ€§èƒ½"
    fi
    
    # æ£€æŸ¥ç£ç›˜ç©ºé—´
    AVAILABLE_DISK=$(df -h "$PROJECT_ROOT" | awk 'NR==2 {print $4}' | sed 's/G//')
    
    if [ "$AVAILABLE_DISK" -lt 20 ]; then
        warn "ç£ç›˜å¯ç”¨ç©ºé—´ä¸è¶³ 20GBï¼Œå»ºè®®æ¸…ç†ç£ç›˜ç©ºé—´"
    fi
    
    info "å¯ç”¨ç£ç›˜ç©ºé—´: ${AVAILABLE_DISK}GB"
}

# ==================== ç¯å¢ƒé…ç½® ====================

setup_environment() {
    log "ğŸ”§ é…ç½®ç¯å¢ƒå˜é‡..."
    
    # åˆ›å»ºç¯å¢ƒé…ç½®æ–‡ä»¶
    ENV_FILE="$PROJECT_ROOT/.env.${ENVIRONMENT}"
    
    if [ ! -f "$ENV_FILE" ]; then
        warn "ç¯å¢ƒé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºé»˜è®¤é…ç½®: $ENV_FILE"
        create_env_file "$ENV_FILE"
    fi
    
    # å¤åˆ¶åˆ°ä¸»é…ç½®æ–‡ä»¶
    cp "$ENV_FILE" "$PROJECT_ROOT/.env"
    
    info "ä½¿ç”¨ç¯å¢ƒé…ç½®: $ENV_FILE"
}

create_env_file() {
    local env_file=$1
    
    cat > "$env_file" << EOF
# AI Travel Planner Environment Configuration
ENVIRONMENT=$ENVIRONMENT

# æ•°æ®åº“é…ç½®
DATABASE_URL=postgresql://travel_user:travel_password_2024@postgres:5432/ai_travel_planner
POSTGRES_DB=ai_travel_planner
POSTGRES_USER=travel_user
POSTGRES_PASSWORD=travel_password_2024

# Redisé…ç½®
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_PASSWORD=redis_password_2024

# APIå¯†é’¥ (è¯·åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä¿®æ”¹)
OPENAI_API_KEY=your_openai_api_key_here
FLIGHT_API_KEY=your_flight_api_key_here
HOTEL_API_KEY=your_hotel_api_key_here
WEATHER_API_KEY=your_weather_api_key_here

# JWTé…ç½®
JWT_SECRET=your_jwt_secret_key_2024

# åº”ç”¨é…ç½®
LOG_LEVEL=info
DEBUG=$( [ "$ENVIRONMENT" = "development" ] && echo "true" || echo "false" )

# ç›‘æ§é…ç½®
ENABLE_MONITORING=$ENABLE_MONITORING
ENABLE_LOGGING=$ENABLE_LOGGING

# æœåŠ¡ç«¯å£é…ç½®
API_GATEWAY_PORT=8000
CHAT_SERVICE_PORT=8001
RAG_SERVICE_PORT=8002
AGENT_SERVICE_PORT=8003
PLANNING_SERVICE_PORT=8004
INTEGRATION_SERVICE_PORT=8005
USER_SERVICE_PORT=8006
FRONTEND_PORT=3000

EOF
    
    info "å·²åˆ›å»ºç¯å¢ƒé…ç½®æ–‡ä»¶: $env_file"
}

# ==================== ç›®å½•ç»“æ„ ====================

setup_directories() {
    log "ğŸ“ åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„..."
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    mkdir -p "$PROJECT_ROOT/logs"
    mkdir -p "$PROJECT_ROOT/logs/services"
    mkdir -p "$PROJECT_ROOT/logs/nginx"
    
    # åˆ›å»ºæ•°æ®ç›®å½•
    mkdir -p "$PROJECT_ROOT/data/documents"
    mkdir -p "$PROJECT_ROOT/data/uploads"
    mkdir -p "$PROJECT_ROOT/data/exports"
    
    # åˆ›å»ºé…ç½®ç›®å½•
    mkdir -p "$PROJECT_ROOT/config/nginx"
    mkdir -p "$PROJECT_ROOT/config/redis"
    mkdir -p "$PROJECT_ROOT/config/qdrant"
    
    # åˆ›å»ºç›‘æ§é…ç½®ç›®å½•
    mkdir -p "$PROJECT_ROOT/monitoring/prometheus"
    mkdir -p "$PROJECT_ROOT/monitoring/grafana/dashboards"
    mkdir -p "$PROJECT_ROOT/monitoring/grafana/datasources"
    mkdir -p "$PROJECT_ROOT/monitoring/logstash/pipeline"
    
    # åˆ›å»ºSSLè¯ä¹¦ç›®å½•
    mkdir -p "$PROJECT_ROOT/ssl"
    
    # è®¾ç½®æƒé™
    chmod 755 "$PROJECT_ROOT/logs"
    chmod 755 "$PROJECT_ROOT/data"
    
    info "ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ"
}

# ==================== é…ç½®æ–‡ä»¶ç”Ÿæˆ ====================

generate_config_files() {
    log "ğŸ“ ç”Ÿæˆé…ç½®æ–‡ä»¶..."
    
    # Nginxé…ç½®
    generate_nginx_config
    
    # Prometheusé…ç½®
    generate_prometheus_config
    
    # Grafanaæ•°æ®æºé…ç½®
    generate_grafana_config
    
    # Logstashé…ç½®
    generate_logstash_config
    
    info "é…ç½®æ–‡ä»¶ç”Ÿæˆå®Œæˆ"
}

generate_nginx_config() {
    cat > "$PROJECT_ROOT/nginx/nginx.conf" << 'EOF'
events {
    worker_connections 1024;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                   '$status $body_bytes_sent "$http_referer" '
                   '"$http_user_agent" "$http_x_forwarded_for"';

    access_log /var/log/nginx/access.log main;
    error_log /var/log/nginx/error.log warn;

    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;

    # Gzipå‹ç¼©
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_types
        text/plain
        text/css
        text/xml
        text/javascript
        application/json
        application/javascript
        application/xml+rss
        application/atom+xml
        image/svg+xml;

    # ä¸Šæ¸¸æœåŠ¡å™¨é…ç½®
    upstream api_gateway {
        server api-gateway:8000;
    }

    upstream frontend {
        server frontend:3000;
    }

    # ä¸»æœåŠ¡å™¨é…ç½®
    server {
        listen 80;
        server_name localhost;

        # å‰ç«¯åº”ç”¨
        location / {
            proxy_pass http://frontend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # APIè·¯ç”±
        location /api/ {
            proxy_pass http://api_gateway;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # WebSocketæ”¯æŒ
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
        }

        # å¥åº·æ£€æŸ¥
        location /health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }
    }
}
EOF
}

generate_prometheus_config() {
    cat > "$PROJECT_ROOT/monitoring/prometheus.yml" << 'EOF'
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  # - "first_rules.yml"
  # - "second_rules.yml"

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'api-gateway'
    static_configs:
      - targets: ['api-gateway:8000']
    metrics_path: '/metrics'

  - job_name: 'chat-service'
    static_configs:
      - targets: ['chat-service:8001']
    metrics_path: '/metrics'

  - job_name: 'rag-service'
    static_configs:
      - targets: ['rag-service:8002']
    metrics_path: '/metrics'

  - job_name: 'agent-service'
    static_configs:
      - targets: ['agent-service:8003']
    metrics_path: '/metrics'

  - job_name: 'planning-service'
    static_configs:
      - targets: ['planning-service:8004']
    metrics_path: '/metrics'

  - job_name: 'integration-service'
    static_configs:
      - targets: ['integration-service:8005']
    metrics_path: '/metrics'

  - job_name: 'user-service'
    static_configs:
      - targets: ['user-service:8006']
    metrics_path: '/metrics'

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres:5432']

  - job_name: 'redis'
    static_configs:
      - targets: ['redis:6379']
EOF
}

generate_grafana_config() {
    # Grafanaæ•°æ®æºé…ç½®
    cat > "$PROJECT_ROOT/monitoring/grafana/datasources/prometheus.yml" << 'EOF'
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: true
EOF

    # Grafanaä»ªè¡¨æ¿é…ç½®
    cat > "$PROJECT_ROOT/monitoring/grafana/dashboards/dashboard.yml" << 'EOF'
apiVersion: 1

providers:
  - name: 'ai-travel-planner'
    orgId: 1
    folder: ''
    type: file
    disableDeletion: false
    updateIntervalSeconds: 10
    options:
      path: /etc/grafana/provisioning/dashboards
EOF
}

generate_logstash_config() {
    cat > "$PROJECT_ROOT/monitoring/logstash/pipeline/logstash.conf" << 'EOF'
input {
  file {
    path => "/usr/share/logstash/logs/*.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}

filter {
  if [message] =~ /^\[/ {
    grok {
      match => { "message" => "\[%{TIMESTAMP_ISO8601:timestamp}\] %{WORD:level}: %{GREEDYDATA:msg}" }
    }
    
    date {
      match => [ "timestamp", "yyyy-MM-dd HH:mm:ss" ]
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch-log:9200"]
    index => "ai-travel-planner-%{+YYYY.MM.dd}"
  }
  
  stdout {
    codec => rubydebug
  }
}
EOF
}

# ==================== æ„å»ºå’Œéƒ¨ç½² ====================

build_images() {
    if [ "$SKIP_BUILD" = true ]; then
        info "è·³è¿‡Dockeré•œåƒæ„å»º"
        return
    fi
    
    log "ğŸ”¨ æ„å»ºDockeré•œåƒ..."
    
    # è·å–Gitæäº¤å“ˆå¸Œä½œä¸ºæ ‡ç­¾
    GIT_HASH=$(git rev-parse --short HEAD 2>/dev/null || echo "latest")
    
    # æ„å»ºå‚æ•°
    BUILD_ARGS="--build-arg ENVIRONMENT=$ENVIRONMENT --build-arg BUILD_VERSION=$GIT_HASH"
    
    # æ„å»ºæœåŠ¡é•œåƒ
    info "æ„å»ºåç«¯æœåŠ¡é•œåƒ..."
    docker-compose build $BUILD_ARGS
    
    # æ„å»ºå‰ç«¯é•œåƒ
    info "æ„å»ºå‰ç«¯é•œåƒ..."
    cd "$PROJECT_ROOT/frontend"
    docker build -t ai-travel-frontend:$GIT_HASH .
    cd "$PROJECT_ROOT"
    
    log "âœ… Dockeré•œåƒæ„å»ºå®Œæˆ"
}

run_tests() {
    if [ "$SKIP_TESTS" = true ]; then
        info "è·³è¿‡æµ‹è¯•è¿è¡Œ"
        return
    fi
    
    log "ğŸ§ª è¿è¡Œæµ‹è¯•..."
    
    # è¿è¡Œåç«¯æµ‹è¯•
    info "è¿è¡Œåç«¯æµ‹è¯•..."
    python -m pytest tests/ -v --tb=short
    
    # è¿è¡Œå‰ç«¯æµ‹è¯•
    info "è¿è¡Œå‰ç«¯æµ‹è¯•..."
    cd "$PROJECT_ROOT/frontend"
    npm test -- --watchAll=false --coverage
    cd "$PROJECT_ROOT"
    
    log "âœ… æµ‹è¯•è¿è¡Œå®Œæˆ"
}

deploy_services() {
    log "ğŸš€ éƒ¨ç½²æœåŠ¡..."
    
    # Docker Composeå‚æ•°
    COMPOSE_ARGS=""
    
    if [ "$FORCE_RECREATE" = true ]; then
        COMPOSE_ARGS="$COMPOSE_ARGS --force-recreate"
    fi
    
    if [ "$CLEANUP_VOLUMES" = true ]; then
        warn "æ¸…ç†æ•°æ®å·ï¼ˆè¿™å°†åˆ é™¤æ‰€æœ‰æ•°æ®ï¼ï¼‰"
        docker-compose down -v
        COMPOSE_ARGS="$COMPOSE_ARGS --renew-anon-volumes"
    fi
    
    # éƒ¨ç½²æ ¸å¿ƒæœåŠ¡
    info "å¯åŠ¨æ ¸å¿ƒæœåŠ¡..."
    docker-compose up -d postgres redis qdrant $COMPOSE_ARGS
    
    # ç­‰å¾…æ•°æ®åº“å¯åŠ¨
    wait_for_service "postgres" "5432"
    wait_for_service "redis" "6379"
    wait_for_service "qdrant" "6333"
    
    # è¿è¡Œæ•°æ®åº“è¿ç§»
    run_migrations
    
    # å¯åŠ¨åç«¯æœåŠ¡
    info "å¯åŠ¨åç«¯æœåŠ¡..."
    docker-compose up -d \
        api-gateway \
        chat-service \
        rag-service \
        agent-service \
        planning-service \
        integration-service \
        user-service \
        $COMPOSE_ARGS
    
    # å¯åŠ¨å‰ç«¯æœåŠ¡
    info "å¯åŠ¨å‰ç«¯æœåŠ¡..."
    docker-compose up -d frontend $COMPOSE_ARGS
    
    # å¯åŠ¨ç›‘æ§æœåŠ¡
    if [ "$ENABLE_MONITORING" = true ]; then
        info "å¯åŠ¨ç›‘æ§æœåŠ¡..."
        docker-compose up -d prometheus grafana jaeger $COMPOSE_ARGS
    fi
    
    # å¯åŠ¨æ—¥å¿—æœåŠ¡
    if [ "$ENABLE_LOGGING" = true ]; then
        info "å¯åŠ¨æ—¥å¿—æœåŠ¡..."
        docker-compose up -d elasticsearch-log logstash kibana $COMPOSE_ARGS
    fi
    
    # å¯åŠ¨å·¥å…·æœåŠ¡
    info "å¯åŠ¨å·¥å…·æœåŠ¡..."
    docker-compose up -d nginx minio adminer redis-commander $COMPOSE_ARGS
    
    # å¯åŠ¨ä»»åŠ¡é˜Ÿåˆ—
    info "å¯åŠ¨ä»»åŠ¡é˜Ÿåˆ—..."
    docker-compose up -d celery-worker celery-beat flower $COMPOSE_ARGS
    
    log "âœ… æœåŠ¡éƒ¨ç½²å®Œæˆ"
}

run_migrations() {
    log "ğŸ“Š è¿è¡Œæ•°æ®åº“è¿ç§»..."
    
    # ç­‰å¾…PostgreSQLå®Œå…¨å¯åŠ¨
    sleep 10
    
    # è¿è¡Œè¿ç§»è„šæœ¬
    docker-compose exec -T postgres psql -U travel_user -d ai_travel_planner -c "
        CREATE SCHEMA IF NOT EXISTS travel;
        
        -- ç”¨æˆ·è¡¨
        CREATE TABLE IF NOT EXISTS travel.users (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            username VARCHAR(50) UNIQUE NOT NULL,
            email VARCHAR(100) UNIQUE NOT NULL,
            password_hash VARCHAR(255) NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        
        -- æ—…è¡Œè®¡åˆ’è¡¨
        CREATE TABLE IF NOT EXISTS travel.plans (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            user_id UUID REFERENCES travel.users(id),
            name VARCHAR(200) NOT NULL,
            description TEXT,
            start_date DATE NOT NULL,
            end_date DATE NOT NULL,
            status VARCHAR(20) DEFAULT 'draft',
            total_cost DECIMAL(10,2) DEFAULT 0,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        
        -- ä¼šè¯è¡¨
        CREATE TABLE IF NOT EXISTS travel.conversations (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            user_id UUID REFERENCES travel.users(id),
            title VARCHAR(200),
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        
        -- åˆ›å»ºç´¢å¼•
        CREATE INDEX IF NOT EXISTS idx_users_email ON travel.users(email);
        CREATE INDEX IF NOT EXISTS idx_plans_user_id ON travel.plans(user_id);
        CREATE INDEX IF NOT EXISTS idx_conversations_user_id ON travel.conversations(user_id);
    "
    
    info "æ•°æ®åº“è¿ç§»å®Œæˆ"
}

# ==================== å¥åº·æ£€æŸ¥ ====================

wait_for_service() {
    local service=$1
    local port=$2
    local max_attempts=30
    local attempt=1
    
    info "ç­‰å¾…æœåŠ¡ $service:$port å¯åŠ¨..."
    
    while [ $attempt -le $max_attempts ]; do
        if docker-compose exec -T "$service" nc -z localhost "$port" 2>/dev/null; then
            info "æœåŠ¡ $service:$port å·²å¯åŠ¨"
            return 0
        fi
        
        echo -n "."
        sleep 2
        attempt=$((attempt + 1))
    done
    
    error "æœåŠ¡ $service:$port å¯åŠ¨å¤±è´¥"
    return 1
}

health_check() {
    log "ğŸ¥ æ‰§è¡Œå¥åº·æ£€æŸ¥..."
    
    local services=(
        "http://localhost:8000/health:APIç½‘å…³"
        "http://localhost:8001/health:èŠå¤©æœåŠ¡"
        "http://localhost:8002/health:RAGæœåŠ¡"
        "http://localhost:8003/health:æ™ºèƒ½ä½“æœåŠ¡"
        "http://localhost:8004/health:è§„åˆ’æœåŠ¡"
        "http://localhost:8005/health:é›†æˆæœåŠ¡"
        "http://localhost:8006/health:ç”¨æˆ·æœåŠ¡"
        "http://localhost:3000:å‰ç«¯åº”ç”¨"
    )
    
    local failed_services=()
    
    for service_info in "${services[@]}"; do
        IFS=':' read -r url name <<< "$service_info"
        
        if curl -f -s --max-time 10 "$url" >/dev/null 2>&1; then
            info "âœ… $name - å¥åº·"
        else
            error "âŒ $name - ä¸å¥åº·"
            failed_services+=("$name")
        fi
    done
    
    if [ ${#failed_services[@]} -eq 0 ]; then
        log "ğŸ‰ æ‰€æœ‰æœåŠ¡å¥åº·æ£€æŸ¥é€šè¿‡"
        return 0
    else
        error "ä»¥ä¸‹æœåŠ¡å¥åº·æ£€æŸ¥å¤±è´¥: ${failed_services[*]}"
        return 1
    fi
}

# ==================== éƒ¨ç½²åé…ç½® ====================

post_deploy_setup() {
    log "âš™ï¸  æ‰§è¡Œéƒ¨ç½²åé…ç½®..."
    
    # åˆ›å»ºé»˜è®¤ç”¨æˆ·
    create_default_user
    
    # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
    initialize_vector_database
    
    # è®¾ç½®ç›‘æ§å‘Šè­¦
    if [ "$ENABLE_MONITORING" = true ]; then
        setup_monitoring_alerts
    fi
    
    info "éƒ¨ç½²åé…ç½®å®Œæˆ"
}

create_default_user() {
    info "åˆ›å»ºé»˜è®¤ç®¡ç†å‘˜ç”¨æˆ·..."
    
    # é€šè¿‡APIåˆ›å»ºé»˜è®¤ç”¨æˆ·
    curl -s -X POST http://localhost:8000/api/v1/users/register \
        -H "Content-Type: application/json" \
        -d '{
            "username": "admin",
            "email": "admin@example.com",
            "password": "admin123"
        }' || warn "é»˜è®¤ç”¨æˆ·åˆ›å»ºå¤±è´¥ï¼Œå¯èƒ½å·²å­˜åœ¨"
}

initialize_vector_database() {
    info "åˆå§‹åŒ–å‘é‡æ•°æ®åº“..."
    
    # åˆ›å»ºé»˜è®¤é›†åˆ
    curl -s -X POST http://localhost:6333/collections/travel_documents \
        -H "Content-Type: application/json" \
        -d '{
            "vectors": {
                "size": 768,
                "distance": "Cosine"
            }
        }' || warn "å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥"
}

setup_monitoring_alerts() {
    info "è®¾ç½®ç›‘æ§å‘Šè­¦..."
    # è¿™é‡Œå¯ä»¥æ·»åŠ Prometheuså‘Šè­¦è§„åˆ™é…ç½®
    # æˆ–è€…Grafanaå‘Šè­¦é…ç½®
}

# ==================== æ˜¾ç¤ºä¿¡æ¯ ====================

show_deployment_info() {
    log "ğŸ“‹ éƒ¨ç½²ä¿¡æ¯"
    
    echo ""
    echo "ğŸŒ è®¿é—®åœ°å€:"
    echo "  ä¸»åº”ç”¨:          http://localhost"
    echo "  APIç½‘å…³:         http://localhost:8000"
    echo "  å‰ç«¯åº”ç”¨:        http://localhost:3000"
    echo ""
    echo "ğŸ”§ ç®¡ç†å·¥å…·:"
    echo "  æ•°æ®åº“ç®¡ç†:      http://localhost:8080"
    echo "  Redisç®¡ç†:       http://localhost:8081"
    echo "  å¯¹è±¡å­˜å‚¨:        http://localhost:9001"
    echo ""
    
    if [ "$ENABLE_MONITORING" = true ]; then
        echo "ğŸ“Š ç›‘æ§æœåŠ¡:"
        echo "  Prometheus:      http://localhost:9090"
        echo "  Grafana:         http://localhost:3001 (admin/admin123)"
        echo "  Jaeger:          http://localhost:16686"
        echo "  Flower:          http://localhost:5555"
        echo ""
    fi
    
    if [ "$ENABLE_LOGGING" = true ]; then
        echo "ğŸ“ æ—¥å¿—æœåŠ¡:"
        echo "  Kibana:          http://localhost:5601"
        echo ""
    fi
    
    echo "ğŸ”‘ é»˜è®¤å‡­æ®:"
    echo "  ç®¡ç†å‘˜ç”¨æˆ·:      admin / admin123"
    echo "  æ•°æ®åº“:          travel_user / travel_password_2024"
    echo "  Redis:           redis_password_2024"
    echo "  MinIO:           minioadmin / minioadmin123"
    echo ""
    
    echo "ğŸ“ é‡è¦è·¯å¾„:"
    echo "  æ—¥å¿—ç›®å½•:        $PROJECT_ROOT/logs"
    echo "  æ•°æ®ç›®å½•:        $PROJECT_ROOT/data"
    echo "  é…ç½®ç›®å½•:        $PROJECT_ROOT/config"
    echo ""
    
    echo "ğŸ› ï¸  å¸¸ç”¨å‘½ä»¤:"
    echo "  æŸ¥çœ‹æ—¥å¿—:        docker-compose logs -f [service]"
    echo "  é‡å¯æœåŠ¡:        docker-compose restart [service]"
    echo "  åœæ­¢æ‰€æœ‰æœåŠ¡:    docker-compose down"
    echo "  æ›´æ–°æœåŠ¡:        $0 -f"
    echo ""
}

# ==================== æ¸…ç†å‡½æ•° ====================

cleanup_on_exit() {
    local exit_code=$?
    
    if [ $exit_code -ne 0 ]; then
        error "éƒ¨ç½²è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œé€€å‡ºç : $exit_code"
        echo ""
        echo "ğŸ” æ•…éšœæ’é™¤å»ºè®®:"
        echo "  1. æŸ¥çœ‹æ—¥å¿—: tail -f $LOG_FILE"
        echo "  2. æ£€æŸ¥æœåŠ¡çŠ¶æ€: docker-compose ps"
        echo "  3. æŸ¥çœ‹æœåŠ¡æ—¥å¿—: docker-compose logs [service]"
        echo "  4. é‡æ–°éƒ¨ç½²: $0 -f"
        echo ""
    fi
    
    exit $exit_code
}

# ==================== å‚æ•°è§£æ ====================

parse_arguments() {
    while [[ $# -gt 0 ]]; do
        case $1 in
            -e|--env)
                ENVIRONMENT="$2"
                shift 2
                ;;
            -s|--skip-build)
                SKIP_BUILD=true
                shift
                ;;
            -t|--skip-tests)
                SKIP_TESTS=true
                shift
                ;;
            -f|--force-recreate)
                FORCE_RECREATE=true
                shift
                ;;
            -c|--cleanup-volumes)
                CLEANUP_VOLUMES=true
                shift
                ;;
            --no-monitoring)
                ENABLE_MONITORING=false
                shift
                ;;
            --no-logging)
                ENABLE_LOGGING=false
                shift
                ;;
            -h|--help)
                show_help
                exit 0
                ;;
            *)
                error "æœªçŸ¥å‚æ•°: $1"
                show_help
                exit 1
                ;;
        esac
    done
    
    # éªŒè¯ç¯å¢ƒå‚æ•°
    if [[ ! "$ENVIRONMENT" =~ ^(development|staging|production)$ ]]; then
        error "æ— æ•ˆçš„ç¯å¢ƒ: $ENVIRONMENT"
        exit 1
    fi
}

# ==================== ä¸»å‡½æ•° ====================

main() {
    # è®¾ç½®ä¿¡å·å¤„ç†
    trap cleanup_on_exit EXIT
    
    # åˆå§‹åŒ–æ—¥å¿—
    echo "AI Travel Planner éƒ¨ç½²å¼€å§‹ - $(date)" > "$LOG_FILE"
    
    log "ğŸš€ AI Travel Planner éƒ¨ç½²è„šæœ¬å¯åŠ¨"
    log "ç¯å¢ƒ: $ENVIRONMENT"
    
    # æ£€æŸ¥å¹¶åˆ›å»ºå¿…è¦ç›®å½•
    setup_directories
    
    # æ£€æŸ¥ç³»ç»Ÿç¯å¢ƒ
    check_prerequisites
    
    # é…ç½®ç¯å¢ƒ
    setup_environment
    
    # ç”Ÿæˆé…ç½®æ–‡ä»¶
    generate_config_files
    
    # æ„å»ºé•œåƒ
    build_images
    
    # è¿è¡Œæµ‹è¯•
    run_tests
    
    # éƒ¨ç½²æœåŠ¡
    deploy_services
    
    # å¥åº·æ£€æŸ¥
    sleep 30  # ç­‰å¾…æœåŠ¡å®Œå…¨å¯åŠ¨
    health_check
    
    # éƒ¨ç½²åé…ç½®
    post_deploy_setup
    
    # æ˜¾ç¤ºéƒ¨ç½²ä¿¡æ¯
    show_deployment_info
    
    log "ğŸ‰ AI Travel Planner éƒ¨ç½²å®Œæˆï¼"
}

# ==================== è„šæœ¬å…¥å£ ====================

# è§£æå‘½ä»¤è¡Œå‚æ•°
parse_arguments "$@"

# åˆ‡æ¢åˆ°é¡¹ç›®æ ¹ç›®å½•
cd "$PROJECT_ROOT"

# æ‰§è¡Œä¸»å‡½æ•°
main 